7/6/23

################# What do I mean by Complexity Scheduling? ################# 

This directory consists of experiments with Complexity Scheduling. What I mean by Complexity Scheduling is any type of training
process that involves scheduling an increase in complexity of either the data or model architecture during training. 

This origin of this idea is predicated on the assumption that all human learning is facilitated by exposure to ideas which are to be learned. 
And that after a certain threshold of exposure has been attained, understanding is unavoidable. The idea stems from an 
observation about when children first learn about swear words and other innapropriate topics for kids. 
Parents protect their kids from hearing about inappropriate concepts when they're young, but they can't entirely protect them 
from this. There is some amount of exposure, albeit small. However, little exposures add up over time. And by the point that 
exposure leads to an understanding, the kid is now an adult and it's no longer inappropriate for them.

The key point being that there are phases of learning that humans go through at different stages of development. At each
of those different stages, a parental guidance tailors dataset for the child to learn from with an emphasis on what is 
currently optimal for them to learn. Learning, then, happens in distinct steps where mastery in a small domain is achieved and then 
difficulty of the learning task is increased in an iterative process.

I aim to apply this same concept to the training of Neural Networks within this directory. 

################# What is in this directory ? ################# 

1_CS_Layerwise_Update.ipynb 

1_CS_Layerwise_Update.ipynb contains an initial implementation of complexity scheduling for model architecture. 
In it a model with one layer is trained and then has a new layer appended to the end of it and then trained again.
This process is repeated until the specified depth is reached.


2_CS_Iterative_Cluster_Introduction.ipynb

2_CS_Iterative_Cluster_Introduction.ipynb contains my initial experimentation with scheduling the complexity of the 
dataset that is being used to train the model. In this notebook, KMeans is used to cluster mnist. Then a clf is trained
on one cluster, then after that training has concluded, the next cluster is concatenated to the dataset. This step of 
retraining after concatenating the next cluster is repeated until the entire dataset is reuinited. This idea was loosely 
inspired by the adversarial training of GANs.


3_CS_Autoencoder_Depth_Scheduling.ipynb

3_CS_Autoencoder_Depth_Scheduling.ipynb is an implementation of the idea from 1_CS_Layerwise_Update.ipynb to do transfer learning
with a simpler model trained on the same task applied to the training of AutoEncoders.